{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_NLP.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM0BiFRq7RLCNb05slcwbvi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saideep3/Test/blob/master/Project_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo27CewmTg80",
        "colab_type": "text"
      },
      "source": [
        "**Mounting Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47z9rrlISOwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b072d3e-6ed5-4a86-f4c1-b31b8315edf0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L9TtvNMUML4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing numpy and pandas\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "pd.set_option('max_colwidth',400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HedIR4_uUU0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cffdb37-2bf7-4bba-bd7b-bcbc9386d424"
      },
      "source": [
        "#Importing sklearn functions\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58P7hludUiO-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "ac71ae62-f0b3-453d-e879-1b5198c69a6c"
      },
      "source": [
        "#Loading data from drive\n",
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/fake_or_real_news.csv')\n",
        "data.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8476</td>\n",
              "      <td>You Can Smell Hillary’s Fear</td>\n",
              "      <td>Daniel Greenfield, a Shillman Journalism Fellow at the Freedom Center, is a New York writer focusing on radical Islam. \\nIn the final stretch of the election, Hillary Rodham Clinton has gone to war with the FBI. \\nThe word “unprecedented” has been thrown around so often this election that it ought to be retired. But it’s still unprecedented for the nominee of a major political party to go war ...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10294</td>\n",
              "      <td>Watch The Exact Moment Paul Ryan Committed Political Suicide At A Trump Rally (VIDEO)</td>\n",
              "      <td>Google Pinterest Digg Linkedin Reddit Stumbleupon Print Delicious Pocket Tumblr \\nThere are two fundamental truths in this world: Paul Ryan desperately wants to be president. And Paul Ryan will never be president. Today proved it. \\nIn a particularly staggering example of political cowardice, Paul Ryan re-re-re-reversed course and announced that he was back on the Trump Train after all. This w...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3608</td>\n",
              "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
              "      <td>U.S. Secretary of State John F. Kerry said Monday that he will stop in Paris later this week, amid criticism that no top American officials attended Sunday’s unity march against terrorism.\\n\\nKerry said he expects to arrive in Paris Thursday evening, as he heads home after a week abroad. He said he will fly to France at the conclusion of a series of meetings scheduled for Thursday in Sofia, Bu...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10142</td>\n",
              "      <td>Bernie supporters on Twitter erupt in anger against the DNC: 'We tried to warn you!'</td>\n",
              "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 The lesson from tonight's Dem losses: Time for Democrats to start listening to the voters. Stop running the same establishment candidates. \\n— People For Bernie (@People4Bernie) November 9, 2016 If Dems didn't want a tight race they shouldn't have worked against Bernie. \\n— Walker Bragman (@WalkerBragman) November 9, 2016 \\nNew York Times columnist ...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>875</td>\n",
              "      <td>The Battle of New York: Why This Primary Matters</td>\n",
              "      <td>It's primary day in New York and front-runners Hillary Clinton and Donald Trump are leading in the polls.\\n\\nTrump is now vowing to win enough delegates to clinch the Republican nomination and prevent a contested convention. But Sens.Ted Cruz, R-Texas, Bernie Sanders, D-Vt., and Ohio Gov. John Kasich and aren't giving up just yet.\\n\\nA big win in New York could tip the scales for both the Repu...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... label\n",
              "0        8476  ...  FAKE\n",
              "1       10294  ...  FAKE\n",
              "2        3608  ...  REAL\n",
              "3       10142  ...  FAKE\n",
              "4         875  ...  REAL\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knwaWmRXWboC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c90654ea-4c7a-4038-c6a7-25b0fb773563"
      },
      "source": [
        "#checking class imbalence\n",
        "data.label.value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "REAL    3171\n",
              "FAKE    3164\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9O1V8efXCx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extracting dependent and independent variables from the data\n",
        "X = data.text\n",
        "label = data.label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXf-X6YPW4ma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "999fba78-dce0-4ca3-cd96-f24f6f4271fb"
      },
      "source": [
        "#Encoding labels, REAL - 1 and FAKE - 0\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelEncoder = LabelEncoder()\n",
        "Y = labelEncoder.fit_transform(label)\n",
        "print(Y)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 ... 0 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJjULg9DYqHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Splitting into test and train sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,random_state=2003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjM63iNVZBaP",
        "colab_type": "text"
      },
      "source": [
        "**Implementing Count Vectorizer on test and train sets**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0olCx0zMZJW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying count vectorization\n",
        "vectorozier = CountVectorizer(stop_words='english')\n",
        "vectorizer_train = vectorozier.fit_transform(X_train.values)\n",
        "vectorizer_test = vectorozier.transform(X_test.values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mU-R7BoZ-dT",
        "colab_type": "text"
      },
      "source": [
        "**Implementing TF-IDF Vectorizer on test and train sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTkxJ6_KZ3A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying TF-IDF vectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_df=0.75) \n",
        "tfVect_train = tfidf.fit_transform(X_train) \n",
        "tfVect_test = tfidf.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JacINNVTtQE2",
        "colab_type": "text"
      },
      "source": [
        "**Hashing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsgkg2hRtQQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying hashing techniques\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "hashing = HashingVectorizer(stop_words='english',n_features=5000)\n",
        "hashing_train = hashing.fit_transform(X_train)\n",
        "hashing_test = hashing.transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eVGmH_gapTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
        "from tensorflow.python.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
        "from tensorflow.python.keras import Sequential\n",
        "\n",
        "#Model Initialization\n",
        "model = Sequential()\n",
        "\n",
        "def model_build(data_x):\n",
        "  #Adding 1st convolution layer\n",
        "  #with 32 filters and kernel_size = 3\n",
        "  model.add(Conv1D(filters = 64, kernel_size=3, activation='relu', input_shape=(data_x.shape[1],1)))\n",
        "  #Adding MaxPool layer with pool_size as 2\n",
        "  model.add(MaxPooling1D(pool_size =2))\n",
        "  #Adding 2nd convolution layer\n",
        "  #with 64 filters and kernel_size = 5\n",
        "  model.add(Conv1D(filters = 128, kernel_size=5, activation='relu'))\n",
        "  #Adding MaxPool layer with pool_size as 2\n",
        "  model.add(MaxPooling1D(pool_size =2))\n",
        "  #Adding 3rd convolution layer\n",
        "  #with 128 filters and kernel_size = 7\n",
        "  model.add(Conv1D(filters = 256, kernel_size=7, activation='relu'))\n",
        "  #Adding MaxPool layer with pool_size as 2\n",
        "  model.add(MaxPooling1D(pool_size =2))\n",
        "  model.add(Flatten())\n",
        "  #Adding Fully connected layer\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  #Adding Dropout layer\n",
        "  model.add(Dropout(0.5))\n",
        "  #Output Layer with 5 outputs\n",
        "  model.add(Dense(1, activation='sigmoid')) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1pI6Z7TbV-N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "c8d7c0fa-53ed-4d94-aecf-23c29af55f40"
      },
      "source": [
        "'''\n",
        "In Case if you are running countverctorization.. change below code to\n",
        "x_train_np = vectorizer_train.toarray()\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "x_test_np = vectorizer_test.toarray()\n",
        "y_test_np = np.array(y_test)\n",
        "x_train = np.expand_dims(x_train_np, axis=2)\n",
        "x_test = np.expand_dims(x_test_np, axis=2)\n",
        "\n",
        "\n",
        "In Case if you are running TF-IDF vectorizer.. change below code to\n",
        "x_train_np = tfVect_train.toarray()\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "x_test_np = tfVect_test.toarray()\n",
        "y_test_np = np.array(y_test)\n",
        "x_train = np.expand_dims(x_train_np, axis=2)\n",
        "x_test = np.expand_dims(x_test_np, axis=2)\n",
        "'''\n",
        "\n",
        "#After performing hashing\n",
        "x_train_np = hashing_train.toarray()\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "x_test_np = hashing_test.toarray()\n",
        "y_test_np = np.array(y_test)\n",
        "x_train = np.expand_dims(x_train_np, axis=2)\n",
        "x_test = np.expand_dims(x_test_np, axis=2)\n",
        "\n",
        "print(x_train_np.shape)\n",
        "#function call\n",
        "model_build(x_train_np)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "#fit model\n",
        "history = model.fit(x_train, y_train_np, epochs=25, batch_size=64)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4434, 5000)\n",
            "Epoch 1/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.4396 - acc: 0.7891\n",
            "Epoch 2/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.2617 - acc: 0.8951\n",
            "Epoch 3/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.1713 - acc: 0.9326\n",
            "Epoch 4/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.1028 - acc: 0.9612\n",
            "Epoch 5/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.0611 - acc: 0.9790\n",
            "Epoch 6/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.0364 - acc: 0.9862\n",
            "Epoch 7/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.0248 - acc: 0.9899\n",
            "Epoch 8/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.0253 - acc: 0.9910\n",
            "Epoch 9/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.0195 - acc: 0.9932\n",
            "Epoch 10/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.0181 - acc: 0.9937\n",
            "Epoch 11/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.0089 - acc: 0.9973\n",
            "Epoch 12/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.0064 - acc: 0.9982\n",
            "Epoch 13/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.0115 - acc: 0.9953\n",
            "Epoch 14/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.0116 - acc: 0.9971\n",
            "Epoch 15/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.0188 - acc: 0.9941\n",
            "Epoch 16/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.0189 - acc: 0.9926\n",
            "Epoch 17/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.0119 - acc: 0.9962\n",
            "Epoch 18/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.0116 - acc: 0.9962\n",
            "Epoch 19/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.0059 - acc: 0.9980\n",
            "Epoch 20/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.0096 - acc: 0.9971\n",
            "Epoch 21/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.0077 - acc: 0.9971\n",
            "Epoch 22/25\n",
            "70/70 [==============================] - 3s 37ms/step - loss: 0.0076 - acc: 0.9973\n",
            "Epoch 23/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.0114 - acc: 0.9953\n",
            "Epoch 24/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.0069 - acc: 0.9973\n",
            "Epoch 25/25\n",
            "70/70 [==============================] - 3s 36ms/step - loss: 0.0065 - acc: 0.9975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxzsnFScuKun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving model\n",
        "model.save(\"6_RumorDetection_2_TT.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1VAbNDbuQCf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "b766e1c7-1f32-4ebd-904e-7b288799b52f"
      },
      "source": [
        "# loading and evaluating  saved model\n",
        "from numpy import loadtxt\n",
        "from tensorflow.python.keras.models import load_model\n",
        " \n",
        "# loading model\n",
        "model1 = load_model('6_RumorDetection_2_TT.h5')\n",
        "#Model summary\n",
        "model1.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 4998, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 2499, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 2495, 128)         41088     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 1247, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 1241, 256)         229632    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 620, 256)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 158720)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               20316288  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 20,587,393\n",
            "Trainable params: 20,587,393\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW_WVlDxpIfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a2deba0-9952-46b9-9b2d-cbd369b612fa"
      },
      "source": [
        "loss, accuracy = model.evaluate(x_test, y_test_np, batch_size=64)\n",
        "print(\"Accuracy \" + str(accuracy))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30/30 [==============================] - 0s 15ms/step - loss: 0.6616 - acc: 0.9016\n",
            "Accuracy 0.901630699634552\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}