{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Assignment 2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNMvvoY9nrJYxkllY56wWVv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saideep3/Test/blob/master/NLP_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf9duxZkEU33",
        "colab_type": "code",
        "outputId": "6c0786e1-0648-4946-9d8e-858947ce2af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Import the pandas library to read our dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Get the train/test split package from sklearn for preparing our dataset to\n",
        "# train and test the model with\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import the numpy library to work with and manipulate the data\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import nltk\n",
        "import random"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM8iAvz5EbWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import data from google drive\n",
        "#data = pd.read_csv('train.tsv', sep='\\t')\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv', sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBj0PW64FQry",
        "colab_type": "code",
        "outputId": "9f4eb1fc-687f-4c00-8413-81fa34694500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re \n",
        "\n",
        "pd.set_option('max_colwidth',400)\n",
        "\n",
        "wordnet = WordNetLemmatizer()\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "punctuations = \"?:!.,;-()\"\n",
        "\n",
        "raw_reviews = data.Phrase.values\n",
        "cleaned_reviews = []\n",
        "\n",
        "for i in range(len(raw_reviews)):\n",
        "  review = str(raw_reviews[i])\n",
        "  review=re.sub('[^a-zA-Z]',' ',review)\n",
        "  review=[wordnet.lemmatize(w) for w in word_tokenize(str(review).lower())]\n",
        "  review=' '.join(review)\n",
        "  cleaned_reviews.append(review)\n",
        "\n",
        "data['cleaned_reviews'] = cleaned_reviews\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>cleaned_reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
              "      <td>1</td>\n",
              "      <td>a series of escapade demonstrating the adage that what is good for the goose is also good for the gander some of which occasionally amuses but none of which amount to much of a story</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "      <td>a series of escapade demonstrating the adage that what is good for the goose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "      <td>a series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...                                                                                                                                                                         cleaned_reviews\n",
              "0         1  ...  a series of escapade demonstrating the adage that what is good for the goose is also good for the gander some of which occasionally amuses but none of which amount to much of a story\n",
              "1         2  ...                                                                                                            a series of escapade demonstrating the adage that what is good for the goose\n",
              "2         3  ...                                                                                                                                                                                a series\n",
              "3         4  ...                                                                                                                                                                                       a\n",
              "4         5  ...                                                                                                                                                                                  series\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LvUQpwMFTBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.cleaned_reviews.values\n",
        "#Converting to categorical\n",
        "Y = to_categorical(data.Sentiment.values)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Intializing Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=4000,stop_words = None, ngram_range=(1,3))\n",
        "X = vectorizer.fit_transform(X)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2dLEfhjFWXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=2003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLqt3kylFYOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train_np = x_train.toarray()\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "x_test_np = x_test.toarray()\n",
        "y_test_np = np.array(y_test)\n",
        "x_train = np.expand_dims(x_train_np, axis=2)\n",
        "x_test = np.expand_dims(x_test_np, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U94z9mguFb5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.nn import Conv1d\n",
        "from torch.nn import MaxPool1d\n",
        "from torch.nn import Flatten\n",
        "from torch.nn import Linear\n",
        "from torch.nn.functional import relu\n",
        "from torch import sigmoid\n",
        "from torch.utils.data import DataLoader, TensorDataset "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj__cXWwf5xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "#custom metrics\n",
        "from keras import backend as K\n",
        "def recall_m(y, y_pred):\n",
        "    tp = K.sum(K.round(K.clip(y * y_pred, 0, 1)))\n",
        "    pp = K.sum(K.round(K.clip(y, 0, 1)))\n",
        "    recall = tp / (pp + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y, y_pred):\n",
        "    tp = K.sum(K.round(K.clip(y * y_pred, 0, 1)))\n",
        "    pp = K.sum(K.round(K.clip(y, 0, 1)))\n",
        "    precision = tp / (pp + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y, y_pred):\n",
        "    precision = precision_m(y, y_pred)\n",
        "    recall = recall_m(y, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llvXmprNFmqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
        "\n",
        "#Model Initialization\n",
        "model = Sequential()\n",
        "\n",
        "def model_build(data_x):\n",
        "  #Adding 1st convolution layer\n",
        "  #with 32 filters and kernel_size = 3\n",
        "  model.add(Conv1D(filters = 64, kernel_size=3, activation='relu', input_shape=(data_x.shape[1],1)))\n",
        "  #Adding MaxPool layer with pool_size as 2\n",
        "  model.add(MaxPooling1D(pool_size =2))\n",
        "  #Adding 2nd convolution layer\n",
        "  #with 64 filters and kernel_size = 5\n",
        "  model.add(Conv1D(filters = 128, kernel_size=5, activation='relu'))\n",
        "  #Adding MaxPool layer with pool_size as 2\n",
        "  model.add(MaxPooling1D(pool_size =2))\n",
        "  #Adding 3rd convolution layer\n",
        "  #with 128 filters and kernel_size = 7\n",
        "  model.add(Conv1D(filters = 256, kernel_size=7, activation='relu'))\n",
        "  #Adding MaxPool layer with pool_size as 2\n",
        "  model.add(MaxPooling1D(pool_size =2))\n",
        "  model.add(Flatten())\n",
        "  #Adding Fully connected layer\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  #Adding Dropout layer\n",
        "  model.add(Dropout(0.5))\n",
        "  #Output Layer with 5 outputs\n",
        "  model.add(Dense(5, activation='softmax')) \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CaUcnHJFnTd",
        "colab_type": "code",
        "outputId": "1bbfe7f0-df7a-4bb8-a652-a2bbf159513c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "#function call\n",
        "model_build(x_train_np)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[metrics.categorical_accuracy,f1_m,precision_m, recall_m])\n",
        "#fit model\n",
        "history = model.fit(x_train, y_train_np, epochs=10, batch_size=128)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "109242/109242 [==============================] - 53s 484us/step - loss: 1.1268 - categorical_accuracy: 0.5523 - f1_m: 0.3560 - precision_m: 0.3560 - recall_m: 0.3560\n",
            "Epoch 2/10\n",
            "109242/109242 [==============================] - 52s 474us/step - loss: 0.9909 - categorical_accuracy: 0.6052 - f1_m: 0.4495 - precision_m: 0.4495 - recall_m: 0.4495\n",
            "Epoch 3/10\n",
            "109242/109242 [==============================] - 52s 473us/step - loss: 0.9273 - categorical_accuracy: 0.6335 - f1_m: 0.5072 - precision_m: 0.5072 - recall_m: 0.5072\n",
            "Epoch 4/10\n",
            "109242/109242 [==============================] - 52s 473us/step - loss: 0.8839 - categorical_accuracy: 0.6499 - f1_m: 0.5412 - precision_m: 0.5412 - recall_m: 0.5412\n",
            "Epoch 5/10\n",
            "109242/109242 [==============================] - 52s 473us/step - loss: 0.8497 - categorical_accuracy: 0.6630 - f1_m: 0.5677 - precision_m: 0.5677 - recall_m: 0.5677\n",
            "Epoch 6/10\n",
            "109242/109242 [==============================] - 52s 473us/step - loss: 0.8264 - categorical_accuracy: 0.6732 - f1_m: 0.5859 - precision_m: 0.5859 - recall_m: 0.5859\n",
            "Epoch 7/10\n",
            "109242/109242 [==============================] - 52s 472us/step - loss: 0.7994 - categorical_accuracy: 0.6826 - f1_m: 0.6035 - precision_m: 0.6035 - recall_m: 0.6035\n",
            "Epoch 8/10\n",
            "109242/109242 [==============================] - 51s 471us/step - loss: 0.7795 - categorical_accuracy: 0.6895 - f1_m: 0.6160 - precision_m: 0.6160 - recall_m: 0.6160\n",
            "Epoch 9/10\n",
            "109242/109242 [==============================] - 52s 472us/step - loss: 0.7627 - categorical_accuracy: 0.6969 - f1_m: 0.6273 - precision_m: 0.6273 - recall_m: 0.6273\n",
            "Epoch 10/10\n",
            "109242/109242 [==============================] - 52s 472us/step - loss: 0.7452 - categorical_accuracy: 0.7025 - f1_m: 0.6372 - precision_m: 0.6372 - recall_m: 0.6372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nA8XlhYMXQb",
        "colab_type": "code",
        "outputId": "191eb01c-16c5-409c-f3b7-e4b2f8414e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Running on test set\n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy \" + str(accuracy) + \":\\n\\ff1_score = \" + str(f1_score) +\n",
        "          \"\\nPrecision = \" + str(precision) + \"\\nRecall = \" + str(recall))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.6466743560169166:\n",
            "\ff1_score = 0.5878507670030533\n",
            "Precision = 0.5878508266051519\n",
            "Recall = 0.5878508266051519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmuu4R9rPypo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving model\n",
        "model.save(\"model.h5\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki9dbNjUP3ac",
        "colab_type": "code",
        "outputId": "5f809093-0763-4807-b2ba-e4c2d9bccd5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "\n",
        "# loading and evaluating  saved model\n",
        "from numpy import loadtxt\n",
        "from keras.models import load_model\n",
        " \n",
        "# loading model\n",
        "model1 = load_model('model.h5', custom_objects={'f1_m': f1_m, 'precision_m':precision_m, 'recall_m':recall_m})\n",
        "#Model summary\n",
        "model1.summary()\n",
        "model1.evaluate(x_test, y_test_np, batch_size=128)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_4 (Conv1D)            (None, 3998, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 1999, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 1995, 128)         41088     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 997, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 991, 256)          229632    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 495, 256)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 126720)            0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16220288  \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 16,491,909\n",
            "Trainable params: 16,491,909\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "46818/46818 [==============================] - 8s 171us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9285249833371354,\n",
              " 0.6466743560602024,\n",
              " 0.5878507669903221,\n",
              " 0.5878508265949669,\n",
              " 0.5878508265949669]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}